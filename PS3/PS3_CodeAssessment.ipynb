{
 "metadata": {
  "name": "",
  "signature": "sha256:ba4703605114417c5ae48766adc76f5dc66b6246976f66a35110672824b144ba"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "PS3a: Code Assessment"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Written by Jill Schulze, INFX 575 - May 12, 2015\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Loading in Libraries"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "import math\n",
      "import scipy\n",
      "import numpy as np\n",
      "from sets import Set\n",
      "import logging, gensim, bz2\n",
      "from gensim import corpora, models, similarities\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import ConditionalFreqDist\n",
      "from nltk.probability import *\n",
      "from nltk.corpus import stopwords\n",
      "from collections import Counter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Loading in Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('abstracts.txt', 'r') as f:\n",
      "    reader = csv.reader(f, dialect='excel', delimiter='\\t')\n",
      "    for row in reader:\n",
      "        print row"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['2458768', 'null']\n",
        "['2408678', 'the recentlydeveloped statistical method known as the bootstrap can be used to place confidence intervals on phylogenies it involves resampling points from ones own data with replacement to create a series of bootstrap samples of the same size as the original data each of these is analyzed and the variation among the resulting estimates taken to indicate the size of the error involved in making estimates from the original data in the case of phylogenies it is argued that the proper method of resampling is to keep all of the original species while sampling characters with replacement under the assumption that the characters have been independently drawn by the systematist and have evolved independently majorityrule consensus trees can be used to construct a phylogeny showing all of the inferred monophyletic groups that occurred in a majority of the bootstrap samples if a group shows up 95 of the time or more the evidence for it is taken to be statistically significant existing computer programs can be used to analyze different bootstrap samples by using weights on the characters the weight of a character being how many times it was drawn in bootstrap sampling when all characters are perfectly compatible as envisioned by hennig bootstrap sampling becomes unnecessary the bootstrap method would show significant evidence for a group if it is defined by three or more characters']\n",
        "['23751', 'unique dna sequences can be determined directly from mouse genomic dna a denaturing gel separates by size mixtures of unlabeled dna fragments from complete restriction and partial chemical cleavages of the entire genome these lanes of dna are transferred and uvcrosslinked to nylon membranes hybridization with a short sup32supplabeled singlestranded probe produces the image of a dna sequence ladder extending from the 3supsup or 5supsup end of one restriction site in the genome numerous different sequences can be obtained from a single membrane by reprobing each band in these sequences represents 3 fg of dna complementary to the probe sequence data from mouse immunoglobulin heavy chain genes from several cell types are presented the genomic sequencing procedures are applicable to the analysis of genetic polymorphisms dna methylation at deoxycytidines and nucleic acidprotein interactions at single nucleotide resolution']\n",
        "['1912352', 'this paper discusses the bias that results from using nonrandomly selected samples to estimate behavioral relationships as an ordinary specification error or omitted variables bias a simple consistent two stage estimator is considered that enables analysts to utilize simple regression methods to estimate behavioral functions by least squares methods the asymptotic distribution of the estimator is derived']\n",
        "['70083', 'a method has been devised for the electrophoretic transfer of proteins from polyacrylamide gels to nitrocellulose sheets the method results in quantitative transfer of ribosomal proteins from gels containing urea for sodium dodecyl sulfate gels the original band pattern was obtained with no loss of resolution but the transfer was not quantitative the method allows detection of proteins by autoradiography and is simpler than conventional procedures the immobilized proteins were detectable by immunological procedures all additional binding capacity on the nitrocellulose was blocked with excess protein then a specific antibody was bound and finally a second antibody directed against the first antibody the second antibody was either radioactively labeled or conjugated to fluorescein or to peroxidase the specific protein was then detected by either autoradiography under uv light or by the peroxidase reaction product respectively in the latter case as little as 100 pg of protein was clearly detectable it is anticipated that the procedure will be applicable to analysis of a wide variety of proteins with specific reactions or ligands']\n",
        "['67376', 'a new method for determining nucleotide sequences in dna is described it is similar to the plus and minus method sanger f  coulson a r 1975 j mol biol 94 441448 but makes use of the 2supsup3supsupdideoxy and arabinonucleoside analogues of the normal deoxynucleoside triphosphates which act as specific chainterminating inhibitors of dna polymerase the technique has been applied to the dna of bacteriophage  x174 and is more rapid and more accurate than either the plus or the minus method']\n",
        "['1745369', 'the commonly observed high diversity of trees in tropical rain forests and corals on tropical reefs is a nonequilibrium state which if not disturbed further will progress toward a lowdiversity equilibrium community this may not happen if gradual changes in climate favor different species if equilibrium is reached a lesser degree of diversity may be sustained by niche diversification or by a compensatory mortality that favors inferior competitors however tropical forests and reefs are subject to severe disturbances often enough that equilibrium may never be attained']\n",
        "['2977928', 'null']\n",
        "['2459010', 'a high number of tree species low density of adults of each species and long distances between conspecific adults are characteristic of many lowland tropical forest habitats i propose that these three traits in large part are the result of the action of predators on seeds and seedlings a model is presented that allows detailed examination of the effect of different predators dispersal agents seedcrop sizes etc on these three traits in short any event that increases the efficiency of the predators at eating seeds and seedlings of a given tree species may lead to a reduction in population density of the adults of that species andor to increased distance between new adults and their parents either event will lead to more space in the habitat for other species of trees and therefore higher total number of tree species provided seed sources are available over evolutionary time as one moves from the wet lowland tropics to the dry tropics or temperate zones the seed and seedling predators in a habitat are hypothesized to be progressively less efficient at keeping one or a few tree species from monopolizing the habitat through competitive superiority this lowered efficiency of the predators is brought about by the increased severity and unpredictability of the physical environment which in turn leads to regular or erratic escape of large seed or seedling cohorts from the predators']\n",
        "['1912934', 'this paper presents a parameter covariance matrix estimator which is consistent even when the disturbances of a linear regression model are heteroskedastic this estimator does not depend on a formal model of the structure of the heteroskedasticity by comparing the elements of the new estimator to those of the usual covariance estimator one obtains a direct test for heteroskedasticity since in the absence of heteroskedasticity the two estimators will be approximately equal but will generally diverge otherwise the test has an appealing least squares interpretation']\n",
        "['1914185', 'this paper presents a critique of expected utility theory as a descriptive model of decision making under risk and develops an alternative model called prospect theory choices among risky prospects exhibit several pervasive effects that are inconsistent with the basic tenets of utility theory in particular people underweight outcomes that are merely probable in comparison with outcomes that are obtained with certainty this tendency called the certainty effect contributes to risk aversion in choices involving sure gains and to risk seeking in choices involving sure losses in addition people generally discard components that are shared by all prospects under consideration this tendency called the isolation effect leads to inconsistent preferences when the same choice is presented in different forms an alternative theory of choice is developed in which value is assigned to gains and losses rather than to final assets and in which probabilities are replaced by decision weights the value function is normally concave for gains commonly convex for losses and is generally steeper for losses than for gains decision weights are generally lower than the corresponding probabilities except in the range of low probabilities overweighting of low probabilities may contribute to the attractiveness of both insurance and gambling']\n",
        "['68319', 'we describe a very sensitive method to detect as antigens the presence of specific proteins within phage plaques or bacterial colonies we coat plastic sheets with antibody molecules expose the sheet to lysed bacteria so that a released antigen can bind and then label the immobilized antigen with radioiodinated antibodies thus the antigen is sandwiched between the antibodies attached to the plastic sheet and those carrying the radioactive label autoradiography then shows the positions of antigencontaining colonies or phage plaques a few molecules of antigen released from each bacterial cell generate an adequate signal']\n",
        "['61400', 'a convenient technique for the partial purification of large quantities of functional polyadenylic acidrich mrna is described the method depends upon annealing polyadenylic acidrich mrna to oligothymidylic acidcellulose columns and its elution with buffers of low ionic strength biologically active rabbit globin mrna has been purified by this procedure and assayed for its ability to direct the synthesis of rabbit globin in a cellfree extract of ascites tumor inasmuch as various mammalian mrnas appear to be rich in polyadenylic acid and can likely be translated in the ascites cellfree extract this approach should prove generally useful as an initial step in the isolation of specific mrnas']\n",
        "['1604132', 'null']\n",
        "['1809766', 'null']\n",
        "['1831029', 'if options are correctly priced in the market it should not be possible to make sure profits by creating portfolios of long and short positions in options and their underlying stocks using this principle a theoretical valuation formula for options is derived since almost all corporate liabilities can be viewed as combinations of options the formula and the analysis that led to it are also applicable to corporate liabilities such as common stock corporate bonds and warrants in particular the formula can be used to derive the discount that should be applied to a corporate bond because of the possibility of default']\n",
        "['1700278', 'a thermostable dna polymerase was used in an in vitro dna amplification procedure the polymerase chain reaction the enzyme isolated from thermus aquaticus greatly simplifies the procedure and by enabling the amplification reaction to be performed at higher temperatures significantly improves the specificity yield sensitivity and length of products that can be amplified singlecopy genomic sequences were amplified by a factor of more than 10 million with very high specificity and dna segments up to 2000 base pairs were readily amplified in addition the method was used to amplify and detect a target dna molecule present only once in a sample of 105 cells']\n",
        "['1603658', 'epoxy embedding methods of glauert and kushida have been modified so as to yield rapid reproducible and convenient embedding methods for electron microscopy the sections are robust and tissue damage is less than with methacrylate embedding']\n",
        "['66230', 'dna can be sequenced by a chemical procedure that breaks a terminally labeled dna molecule partially at each repetition of a base the lengths of the labeled fragments then identify the positions of that base we describe reactions that cleave dna preferentially at guanines at adenines at cytosines and thymines equally and at cytosines alone when the products of these four reactions are resolved by size by electrophoresis on a polyacrylamide gel the dna sequence can be read from the pattern of radioactive bands the technique will permit sequencing of at least 100 bases from the point of labeling']\n",
        "['2461605', 'comparative studies of the relationship between two phenotypes or between a phenotype and an environment are frequently carried out by invalid statistical methods most regression correlation and contingency table methods including nonparametric methods assume that the points are drawn independently from a common distribution when species are taken from a branching phylogeny they are manifestly nonindependent use of a statistical method that assumes independence will cause overstatement of the significance in hypothesis tests some illustrative examples of these phenomena have been given and limitations of previous proposals of ways to correct for the nonindependence have been discussed a method of correcting for the phylogeny has been proposed it requires that we know both the tree topology and the branch lengths and that we be willing to allow the characters to be modeled by brownian motion on a linear scale given these conditions the phylogeny specifies a set of contrasts among species contrasts that are statistically independent and can be used in regression or correlation studies the considerable barriers to making practical use of this technique have been discussed']\n",
        "['2228949', 'null']\n",
        "['1942661', 'pseudoreplication is defined as the use of inferential statistics to test for treatment effects with data from experiments where either treatments are not replicated though samples may be or replicates are not statistically independent in anova terminology it is the testing for treatment effects with an error term inappropriate to the hypothesis being considered scrutiny of 176 experimental studies published between 1960 and the present revealed that pseudoreplication occurred in 27 of them or 48 of all such studies that applied inferential statistics the incidence of pseudoreplication is especially high in studies of marine benthos and small mammals the critical features of controlled experimentation are reviewed nondemonic intrusion is defined as the impingement of chance events on an experiment in progress as a safeguard against both it and preexisting gradients interspersion of treatments is argued to be an obligatory feature of good design especially in small experiments adequate interspersion can sometimes be assured only by dispensing with strict randomization procedures comprehension of this conflict between interspersion and randomization is aided by distinguishing prelayout or conventional and layoutspecific alpha probability of type i error suggestions are offered to statisticians and editors of ecological journals as to how ecologists understanding of experimental design and statistics might be improved']\n",
        "['1913738', 'this paper concerns utility functions for money a measure of risk aversion in the small the risk premium or insurance premium for an arbitrary risk and a natural concept of decreasing risk aversion are discussed and related to one another risks are also considered as a proportion of total assets']\n",
        "['724810', 'null']\n",
        "['2096934', 'null']\n",
        "['2825234', 'this review organizes ideas on the evolution of life histories the key lifehistory traits are brood size size of young the age distribution of reproductive effort the interaction of reproductive effort with adutl mortality and the variation in these traits among an individuals progeny the general theoretical problem is to predict which combinations of traits will evolve in organisms living in specified circumstances first consider single traits theorists have made the following predictions 1 where adult exceeds juvenile mortality the organism should reproduce only once in its lifetime where juvenile exceeds adult mortality the organism should reproduce several times 2 brood size should maximize the number of young surviving to maturity summed over the lifetime of the parent but when optimum broodsize varies unpredictably in time smaller broods should be favored because they decrease the chances of total failure on a given attempt 3 in expanding populations selection should minimize age at maturity in stable populations when reproductive success depends on size age or social status or when adult exceeds juvenile mortality then maturation should be delayed as it should be in declining populations 4 young should increase in size at birth with increased predation risk and decrease in size with increased resource availability theorists have also predicted that only particular combinations of traits should occur in specified circumstances 5 in growing populations age at maturity should be minimized reproductive effort concentrated early in life and brood size increased 6 one view holds that in stable environments late maturity multiple broods a few large young parental care and small reproductive efforts should be favored kselection in fluctuating environments early maturity many small young reduced parental care and large reproductive efforts should be favored rselection 7 but another view holds that when juvenile mortality fluctuates more than adult mortality the traits associated with stable and fluctuating environments should be reversed we need experiments that test the assumptions and predictions reviewed here more comprehensive theory that makes more readily falsifiable predictions and examination of different definitions of fitness']\n",
        "['9298', 'a simple and rapid method for transferring rna from agarose gels to nitrocellulose paper for blot hybridization has been developed polyasupsup and ribosomal rnas transfer efficiently to nitrocellulose paper in high salt 3 m nacl03 m trisodium citrate after denaturation with glyoxal and 50 volvol dimethyl sulfoxide rna also binds to nitrocellulose after treatment with methylmercuric hydroxide the method is sensitive about 50 pg of specific mrna per band is readily detectable after hybridization with high specific activity probes 10sup8sup cpm g the rna is stably bound to the nitrocellulose paper by this procedure allowing removal of the hybridized probes and rehybridization of the rna blots without loss of sensitivity the use of nitrocellulose paper for the analysis of rna by blot hybridization has several advantages over the use of activated paper diazobenzyloxymethylpaper the method is simple inexpensive reproducible and sensitive in addition denaturation of dna with glyoxal and dimethyl sulfoxide promotes transfer and retention of small dnas 100 nucleotides and larger to nitrocellulose paper a related method is also described for dotting rna and dna directly onto nitrocellulose paper treated with a high concentration of salt under these conditions denatured dna of less than 200 nucleotides is retained and hybridizes efficiently']\n",
        "['2626876', 'null']\n",
        "['2459379', 'it is suggested that local animal species diversity is related to the number of predators in the system and their efficiency in preventing single species from monopolizing some important limiting requisite in the marine rocky intertidal this requisite usually is space where predators capable of preventing monopolies are missing or are experimentally removed the systems become less diverse on a local scale no relationship between latitude 10supsup to 49supsup n and diversity was found on a geographic scale an increased stability of annual production may lead to an increased capacity for systems to support higherlevel carnivores hence tropical or other ecosystems are more diverse and are characterized by disproportionately more carnivores']\n",
        "['2409177', 'null']\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('groups.txt', 'r') as f:\n",
      "    reader = csv.reader(f, dialect='excel', delimiter='\\t')\n",
      "    for row in reader:\n",
      "        print row"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['pID']\n",
        "['2408678', '1']\n",
        "['2459379', '1']\n",
        "['2409177', '1']\n",
        "['2461605', '1']\n",
        "['2458768', '1']\n",
        "['1745369', '1']\n",
        "['2825234', '1']\n",
        "['2096934', '1']\n",
        "['2459010', '1']\n",
        "['1942661', '1']\n",
        "['67376', '2']\n",
        "['66230', '2']\n",
        "['70083', '2']\n",
        "['1604132', '2']\n",
        "['1700278', '2']\n",
        "['1603658', '2']\n",
        "['9298', '2']\n",
        "['61400', '2']\n",
        "['23751', '2']\n",
        "['68319', '2']\n",
        "['2626876', '3']\n",
        "['1912934', '3']\n",
        "['2228949', '3']\n",
        "['724810', '3']\n",
        "['1912352', '3']\n",
        "['2977928', '3']\n",
        "['1809766', '3']\n",
        "['1914185', '3']\n",
        "['1913738', '3']\n",
        "['1831029', '3']\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('stopwords.txt', 'r') as f:\n",
      "    reader = csv.reader(f, dialect='excel', delimiter='\\t')\n",
      "    for stopwords in reader:\n",
      "        print stopwords\n",
      "\n",
      "stopset = set(stopwords)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['stopwords = [\"all\",\"just\",\"being\",\"over\",\"both\",\"through\",\"yourselves\",\"its\",\"before\",\"herself\",\"had\",\"should\",\"to\",\"only\",\"under\",\"ours\",\"has\",\"do\",\"them\",\"his\",\"very\",\"they\",\"not\",\"during\",\"now\",\"him\",\"nor\",\"did\",\"this\",\"she\",\"each\",\"further\",\"where\",\"few\",\"because\",\"doing\",\"some\",\"are\",\"our\",\"ourselves\",\"out\",\"what\",\"for\",\"while\",\"does\",\"above\",\"between\",\"t\",\"be\",\"we\",\"who\",\"were\",\"here\",\"hers\",\"by\",\"on\",\"about\",\"of\",\"against\",\"s\",\"or\",\"own\",\"into\",\"yourself\",\"down\",\"your\",\"from\",\"her\",\"their\",\"there\",\"been\",\"whom\",\"too\",\"themselves\",\"was\",\"until\",\"more\",\"himself\",\"that\",\"but\",\"don\",\"with\",\"than\",\"those\",\"he\",\"me\",\"myself\",\"these\",\"up\",\"will\",\"below\",\"can\",\"theirs\",\"my\",\"and\",\"then\",\"is\",\"am\",\"it\",\"an\",\"as\",\"itself\",\"at\",\"have\",\"in\",\"any\",\"if\",\"again\",\"no\",\"when\",\"same\",\"how\",\"other\",\"which\",\"you\",\"after\",\"most\",\"such\",\"why\",\"a\",\"off\",\"i\",\"yours\",\"so\",\"the\",\"having\",\"once\"]']\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "freqDist1 = FreqDist(row)\n",
      "print freqDist1.items()\n",
      "\n",
      "# for key, value in freqDist3.items(): \n",
      "#     if key not in freqDist1.keys(): \n",
      "#         freqDist1[key] = value"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('1831029', 1), ('3', 1)]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Calculating Jargon Distance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While LDA has successfully showed change in scientific fields over time, jargon distance peers deeper into the differences between scientific fields and examines \"cultural holes\" (http://www.knowledgelab.org/docs/finding-cultural-holes.pdf). Cultural holes widen as jargon increases in a particular group, essentially increasing the barrier to understanding for any outsiders. If jargon can be quantified, which the researchers who created the measurement formula believe it can, then scientists can work towards resolving unnecessary impediments in the language of their field. In summary, researchers who have studied the jargon distance of academic articles have concluded jargon isn't all that helpful or useful--and many students who are far from being experts in any field might be quick to agree!\n",
      "\n",
      "Jargon distance, and the replication of it in this particular example, is measuring the difference in probability distributions of a random variable (x). Given the data set, x could be any unigram found in a document within the corpus.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# JARGON DISTANCE\n",
      "# Look at the frequency of unigrams in a document\n",
      "# Calculate the probability of a unigram in the group of documents\n",
      "# Find probability of the unigram for the entire corpus\n",
      "# Calculate Shannon Entropy for a GROUP of documents\n",
      "# Then measure entropy across the 2 different groups, the \"cross_entropy\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_i = {'beautiful': 1, 'sound': 1, 'play': 1, 'sure': 1, 'like': 1, 'I': 1, 'computers': 1, 'problems': 1, 'Pianos': 2, 'solve': 1, 'piano': 1} #'City': 0, 'Seattle': 0, 'May': 0, 'familiar': 0, 'list': 0, 'Community': 0, 'centers': 0, 'riots': 0, 'councils': 0, 'officials': 0, 'candidates': 0, 'election': 0, 'lot': 0, 'political': 0, 'The': 0, '2016': 0, 'Local': 0, 'Day': 0, 'presidential': 0}\n",
      "\n",
      "data_j = {'City': 1, 'Seattle': 2, 'May': 1, 'familiar': 1, 'list': 1, 'Community': 1, 'centers': 1, 'riots': 1, 'councils': 1, 'officials': 1, 'candidates': 1, 'election': 1, 'lot': 1, 'political': 1, 'The': 1, '2016': 1, 'Local': 1, 'Day': 1, 'presidential': 1}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Frequency Distribution for Group i \n",
      "freqDist1 = FreqDist(data_i)\n",
      "print freqDist1.items()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('beautiful', 1), ('sound', 1), ('play', 1), ('sure', 1), ('like', 1), ('I', 1), ('computers', 1), ('problems', 1), ('Pianos', 2), ('solve', 1), ('piano', 1)]\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Probability Distribution for Group i\n",
      "# probDist_i = []\n",
      "# total_words = float(sum(freqDist1.values()))\n",
      "# for key in freqDist1.keys():\n",
      "#     freqDist1[key] = freqDist1[key]/total_words\n",
      "#     probDist_i.append(freqDist1[key])\n",
      "# print probDist_i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Frequency Distribution for Group j\n",
      "freqDist2 = FreqDist(data_j)\n",
      "print freqDist2.items()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('City', 1), ('councils', 1), ('familiar', 1), ('officials', 1), ('election', 1), ('political', 1), ('The', 1), ('Local', 1), ('centers', 1), ('presidential', 1), ('Seattle', 2), ('May', 1), ('list', 1), ('Community', 1), ('riots', 1), ('candidates', 1), ('lot', 1), ('2016', 1), ('Day', 1)]\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Probability Distribution for Group j\n",
      "# probDist_j = []\n",
      "# total_words_j = float(sum(freqDist2.values()))\n",
      "# for key in freqDist2.keys():\n",
      "#     freqDist2[key] = freqDist2[key]/total_words_j\n",
      "#     probDist_j.append(freqDist2[key])\n",
      "# print probDist_j"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus_data = {'beautiful': 1, 'sound': 1, 'play': 1, 'sure': 1, 'like': 1, 'I': 1, 'computers': 1, 'problems': 1, 'Pianos': 2, 'The': 1, 'solve': 1, 'piano': 1,'City': 1, 'councils': 1, 'familiar': 1, 'officials': 1, 'election': 1, 'political': 1, 'Local': 1, 'centers': 1, 'presidential': 1, 'Seattle': 2, 'May': 1, 'list': 1, 'Community': 1, 'riots': 1, 'candidates': 1, 'lot': 1, '2016': 1, 'Day': 1}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Frequency Distribution for Corpus\n",
      "freqDist3 = FreqDist(corpus_data)\n",
      "print freqDist3.items()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('beautiful', 1), ('City', 1), ('play', 1), ('sure', 1), ('councils', 1), ('familiar', 1), ('problems', 1), ('list', 1), ('officials', 1), ('election', 1), ('Pianos', 2), ('The', 1), ('Local', 1), ('centers', 1), ('presidential', 1), ('sound', 1), ('Seattle', 2), ('like', 1), ('May', 1), ('computers', 1), ('political', 1), ('Community', 1), ('riots', 1), ('I', 1), ('candidates', 1), ('lot', 1), ('solve', 1), ('piano', 1), ('2016', 1), ('Day', 1)]\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# Probability Distribution for Corpus\n",
      "# probDist_corpus = []\n",
      "# total_words_corp = float(sum(freqDist3.values()))\n",
      "# for key in freqDist3.keys():\n",
      "#     freqDist3[key] = freqDist3[key]/float(total_words_corp)\n",
      "#     probDist_corpus.append(freqDist3[key])\n",
      "# print probDist_corpus"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CODEBOOK PSI\n",
      "for key, value in freqDist3.items(): \n",
      "    if key not in freqDist1.keys(): \n",
      "        freqDist1[key] = value"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CODEBOOK PSJ\n",
      "for key, value in freqDist3.items(): \n",
      "    if key not in freqDist2.keys(): \n",
      "        freqDist2[key] = value"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# UPDATED PROBABILITIES \n",
      "print freqDist1.items()\n",
      "print\n",
      "print freqDist2.items()\n",
      "print\n",
      "print freqDist3.items()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('beautiful', 1), ('City', 1), ('councils', 1), ('officials', 1), ('election', 1), ('Local', 1), ('centers', 1), ('May', 1), ('political', 1), ('riots', 1), ('candidates', 1), ('lot', 1), ('piano', 1), ('2016', 1), ('Day', 1), ('play', 1), ('sure', 1), ('I', 1), ('familiar', 1), ('problems', 1), ('The', 1), ('presidential', 1), ('sound', 1), ('like', 1), ('Pianos', 2), ('computers', 1), ('list', 1), ('Community', 1), ('solve', 1), ('Seattle', 2)]\n",
        "\n",
        "[('beautiful', 1), ('City', 1), ('play', 1), ('sure', 1), ('councils', 1), ('familiar', 1), ('problems', 1), ('officials', 1), ('election', 1), ('political', 1), ('The', 1), ('Local', 1), ('centers', 1), ('presidential', 1), ('sound', 1), ('Seattle', 2), ('I', 1), ('like', 1), ('May', 1), ('computers', 1), ('list', 1), ('Community', 1), ('riots', 1), ('Pianos', 2), ('candidates', 1), ('lot', 1), ('solve', 1), ('piano', 1), ('2016', 1), ('Day', 1)]\n",
        "\n",
        "[('beautiful', 1), ('City', 1), ('play', 1), ('sure', 1), ('councils', 1), ('familiar', 1), ('problems', 1), ('list', 1), ('officials', 1), ('election', 1), ('Pianos', 2), ('The', 1), ('Local', 1), ('centers', 1), ('presidential', 1), ('sound', 1), ('Seattle', 2), ('like', 1), ('May', 1), ('computers', 1), ('political', 1), ('Community', 1), ('riots', 1), ('I', 1), ('candidates', 1), ('lot', 1), ('solve', 1), ('piano', 1), ('2016', 1), ('Day', 1)]\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that the probability distribution of both document groups, or \"codebooks,\" have been calculated, we can begin to calculate entropy. Shannon entropy is the result of the expected message length per phrase (a unigram in this case) given the probability distribution of a random variable (x). First, we compute the optimum message length in the first group of documents, group i.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Shannon Entropy Code Citation: http://code.activestate.com/recipes/577476-shannon-entropy-calculation/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "shannon_entropy_i = -sum([x * np.log2(x) for x in freqDist1.itervalues()])\n",
      "print 'Shannon Entropy Group i'\n",
      "print shannon_entropy_i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Shannon Entropy Group i\n",
        "-4.0\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "shannon_entropy_groupj = -sum([x * np.log2(x) for x in freqDist2.itervalues()])\n",
      "print 'Shannon Entropy Group j'\n",
      "print shannon_entropy_groupj"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Shannon Entropy Group j\n",
        "-4.0\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cross_entropy(probDist1, probDist2):\n",
      "    x = []\n",
      "    for k, v in freqDist1.iteritems():\n",
      "        x.append(freqDist1[k] * np.log2(freqDist2[k]))\n",
      "    cross_entropy = -sum(x) \n",
      "    return cross_entropy\n",
      "XEntropy = cross_entropy(freqDist1.items(), freqDist2.items())\n",
      "print 'Cross Entropy from Group i to j'\n",
      "print XEntropy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cross Entropy from Group i to j\n",
        "-4.0\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can conclude a few things from the Shannon Entropy, the optimum message length of a random variable. First, the output of Shannon Entropy gives us the minimum number of bits, if rounded up, required to encode each symbol from group i, which is 7 bits for this example. (Note that entropy is rounded up since bits must be expressed in whole numbers only.) \n",
      "\n",
      "Cross Entropy, measuring messages between different groups discussing different topics results in a larger entropy or longer message length. This means a writer and reader from different fields will need to use longer messages (and more bits) to communicate and achieve a similar level of understanding. Intuitively, when people are from different groups and we see entropy  increase, this correlates to increasing jargon distance and more costly communication.\n",
      "\n",
      "It is interesting to note that evaluating cultural holes will vary by the direction of comparison. For example, comparing group i to group j will result in a different answer than comparing group j to group i. This can be interpreted as one group having less jargon, a smaller cultural hole, and easier to understand language for outsiders. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Calculating Efficency of Communication"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eff_comm = (shannon_entropy_i/XEntropy)\n",
      "print 'Efficiency Distance:', eff_comm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Efficiency Distance: 1.0\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our data set is not ideal for this particular type of Jargon Distance measurement, and this is evident from near perfect communication efficiency--even across subject matter fields. Regardless, we can still see somewhat of a point demonstrated: jargon does exist and it impedes perfect communication. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Calculating a Cultural Hole"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hole = 1 - eff_comm\n",
      "print 'Cultural Hole:', hole"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cultural Hole: 0.0\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The final step of this white paper process is to calculate the cultural hole between our 2 groups, which is equivalent to about 13%. If this result was true of a larger corpus, researchers may be able to conclude a great deal of effort had been put into the writing to eliminate jargon and invite outsiders into the discussion. \n",
      "\n",
      "Therein lies the major difference between computing LDA and the Cultural Hole of text-based data. LDA will cluster text into topics and produce the probability of a word's fitting into a particular topic, but measuring Cultural Holes looks at the desparity between documents. Cultural Holes and Jargon Distance care more about the collective whole--an entire document, a group of documents--and how these groups compare to one another. \n",
      "\n",
      "While LDA has been successful in measuring change of scientific fields, probably by measuring what topics and words are appearing most frequently over time, Jargon Distance measures how these words compare across fields. This becomes practical and purposeful when scientists, while aware they are progressing in the research and understanding of their own field, lose sight of how siloed and insular they are becoming to their larger scientific network or sphere of influence. Becoming aware of the use of jargon and trying to limit its appearance may stand to attract more people to a field that has previously been unapproachable or has grown increasingly complex over the years. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}