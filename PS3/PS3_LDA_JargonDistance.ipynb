{
 "metadata": {
  "name": "",
  "signature": "sha256:1769a5cd163c9c295254401324cf10ba49694bd2aa0c9a19084fd9e58b031e3f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Problem Set 3a: LDA and Jargon Distance"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Written by Jill Schulze, INFX 575 - May 8, 2015\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Comments have been written throughout this notebook to address the results of LDA, the process of calculating Jargon Distance and Cultural Holes, and to compare the methods of LDA to Jargon Distance. These comments are in lieu of a separate paper, so the reader can more easily track comments, explanations, and the corresponding code. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Loading in Libraries"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "import scipy\n",
      "import numpy as np\n",
      "from sets import Set\n",
      "import logging, gensim, bz2\n",
      "from gensim import corpora, models, similarities\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.probability import ConditionalFreqDist\n",
      "from nltk.probability import *\n",
      "from nltk.corpus import stopwords\n",
      "from collections import Counter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Loading in Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_1 = open('text_1.txt', 'r')\n",
      "data_1.read()\n",
      "\n",
      "stopset = set(stopwords.words('english'))\n",
      "\n",
      "with open('text_1.txt', 'r') as text_file:\n",
      "    text = text_file.read()\n",
      "    tokens = nltk.word_tokenize(str(text))\n",
      "    tokens = [w for w in tokens if not w in stopset]\n",
      "    print tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['I', 'play', 'piano']\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_2 = open('text_2.txt', 'r')\n",
      "data_2.read()\n",
      "\n",
      "with open('text_2.txt', 'r') as text_file2:\n",
      "    text_2 = text_file2.read()\n",
      "    tokens_2 = nltk.word_tokenize(str(text_2))\n",
      "    tokens_2 = [w for w in tokens_2 if not w in stopset]\n",
      "    print tokens_2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['Pianos', 'solve', 'problems', 'like', 'computers', 'Pianos', 'sure', 'sound', 'beautiful']\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_3 = open('text_3.txt', 'r')\n",
      "data_3.read()\n",
      "\n",
      "with open('text_3.txt', 'r') as text_file3:\n",
      "    text_3 = text_file3.read()\n",
      "    tokens_3 = nltk.word_tokenize(str(text_3))\n",
      "    tokens_3 = [w for w in tokens_3 if not w in stopset]\n",
      "    print tokens_3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['The', '2016', 'presidential', 'election', 'list', 'familiar', 'candidates']\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_4 = open('text_4.txt', 'r')\n",
      "data_4.read()\n",
      "\n",
      "with open('text_4.txt', 'r') as text_file4:\n",
      "    text_4 = text_file4.read()\n",
      "    tokens_4 = nltk.word_tokenize(str(text_4))\n",
      "    tokens_4 = [w for w in tokens_4 if not w in stopset]\n",
      "    print tokens_4"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['May', 'Day', 'Seattle', 'lot', 'political', 'riots']\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_5 = open('text_5.txt', 'r')\n",
      "data_5.read()\n",
      "\n",
      "with open('text_5.txt', 'r') as text_file5:\n",
      "    text_5 = text_file5.read()\n",
      "    tokens_5 = nltk.word_tokenize(str(text_5))\n",
      "    tokens_5 = [w for w in tokens_5 if not w in stopset]\n",
      "    print tokens_5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['City', 'councils', 'Seattle', 'Local', 'officials', 'Community', 'centers']\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Forming dictionaries and a corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Forming dictionary for entire document corpus\n",
      "texts = [tokens, tokens_2, tokens_3, tokens_4, tokens_5]\n",
      "dictionary = corpora.Dictionary(texts)\n",
      "print(dictionary)\n",
      "print \n",
      "print(dictionary.token2id)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dictionary(30 unique tokens: [u'beautiful', u'City', u'councils', u'officials', u'election']...)\n",
        "\n",
        "{u'beautiful': 4, u'City': 24, u'councils': 28, u'officials': 29, u'election': 14, u'Local': 26, u'centers': 27, u'May': 19, u'political': 22, u'riots': 23, u'candidates': 13, u'lot': 21, u'piano': 1, u'2016': 11, u'Day': 18, u'play': 2, u'sure': 10, u'I': 0, u'familiar': 15, u'problems': 7, u'The': 12, u'presidential': 17, u'sound': 9, u'like': 6, u'Pianos': 3, u'computers': 5, u'list': 16, u'Community': 25, u'solve': 8, u'Seattle': 20}\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Forming dictionary for group 1\n",
      "group1 = [tokens, tokens_2]\n",
      "dictionary_group1 = corpora.Dictionary(group1)\n",
      "print(dictionary_group1)\n",
      "print \n",
      "print(dictionary_group1.token2id)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dictionary(11 unique tokens: [u'beautiful', u'sound', u'play', u'sure', u'like']...)\n",
        "\n",
        "{u'beautiful': 4, u'sound': 9, u'play': 2, u'sure': 10, u'like': 6, u'I': 0, u'computers': 5, u'problems': 7, u'Pianos': 3, u'solve': 8, u'piano': 1}\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Group 1 word count\n",
      "data = ['I','play', 'piano', 'Pianos', 'solve', 'problems', 'like', 'computers', 'Pianos', 'sure', 'sound', 'beautiful']\n",
      "count_unigrams = Counter()\n",
      "for elem in data:\n",
      "    count_unigrams[elem] += 1\n",
      "countUNI = dict(count_unigrams)\n",
      "print countUNI"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'sound': 1, 'beautiful': 1, 'play': 1, 'sure': 1, 'like': 1, 'I': 1, 'computers': 1, 'problems': 1, 'Pianos': 2, 'solve': 1, 'piano': 1}\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Dictionary for group 2\n",
      "group2 = [tokens_3, tokens_4, tokens_5]\n",
      "dictionary_group2 = corpora.Dictionary(group2)\n",
      "print(dictionary_group2)\n",
      "print \n",
      "print(dictionary_group2.token2id)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dictionary(19 unique tokens: [u'City', u'Seattle', u'May', u'familiar', u'list']...)\n",
        "\n",
        "{u'City': 13, u'Seattle': 9, u'May': 8, u'familiar': 4, u'list': 5, u'Community': 14, u'centers': 16, u'riots': 12, u'councils': 17, u'officials': 18, u'candidates': 2, u'election': 3, u'lot': 10, u'political': 11, u'The': 1, u'2016': 0, u'Local': 15, u'Day': 7, u'presidential': 6}\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Group 2 word count\n",
      "data2 = ['City','Seattle','May','familiar','list','Community','centers','riots','councils','Seattle','officials','candidates','election','lot','political','2016','Local','Day','presidential']\n",
      "count_unigrams2 = Counter()\n",
      "for elem in data2:\n",
      "    count_unigrams2[elem] += 1\n",
      "countUNI2 = dict(count_unigrams2)\n",
      "print countUNI2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'City': 1, 'councils': 1, 'familiar': 1, 'officials': 1, 'election': 1, 'political': 1, 'Local': 1, 'centers': 1, 'presidential': 1, 'Seattle': 2, 'May': 1, 'list': 1, 'Community': 1, 'riots': 1, 'candidates': 1, 'lot': 1, '2016': 1, 'Day': 1}\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Corpus for all groups\n",
      "corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "print(corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[(0, 1), (1, 1), (2, 1)], [(3, 2), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)], [(11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)], [(18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1)], [(20, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1)]]\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Running LDA for Toy Data and LDA Results Discussion"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "LDA is a helpful learning algorithm to employ on text-based documents in order to automatically and jointly cluster words into topics and documents into mixtures of topics. Similar to the white paper we are replicating for class, LDA has been successfully applied to model change in scientific fields over time (http://www.knowledgelab.org/docs/finding-cultural-holes.pdf). We will use LDA as the foundation for understanding the topics in our \"toy\" data set for implementing a new type of calculation: jargon distance. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# LDA for all documents in corpus\n",
      "lda = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=20)\n",
      "print lda.show_topics(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'0.033*Seattle + 0.033*I + 0.033*piano + 0.033*play + 0.033*May + 0.033*Day + 0.033*riots + 0.033*political + 0.033*lot + 0.033*councils', u'0.183*I + 0.183*piano + 0.183*play + 0.017*Seattle + 0.017*May + 0.017*Day + 0.017*riots + 0.017*political + 0.017*lot + 0.017*councils', u'0.033*Seattle + 0.033*I + 0.033*piano + 0.033*play + 0.033*May + 0.033*Day + 0.033*riots + 0.033*political + 0.033*lot + 0.033*councils', u'0.110*election + 0.110*presidential + 0.110*2016 + 0.110*The + 0.110*candidates + 0.110*familiar + 0.110*list + 0.010*Seattle + 0.010*piano + 0.010*play', u'0.175*Pianos + 0.092*sound + 0.092*solve + 0.092*problems + 0.092*beautiful + 0.092*like + 0.092*computers + 0.092*sure + 0.008*Seattle + 0.008*I', u'0.033*Seattle + 0.033*I + 0.033*piano + 0.033*play + 0.033*May + 0.033*Day + 0.033*riots + 0.033*political + 0.033*lot + 0.033*councils', u'0.033*Seattle + 0.033*I + 0.033*piano + 0.033*play + 0.033*May + 0.033*Day + 0.033*riots + 0.033*political + 0.033*lot + 0.033*councils', u'0.110*officials + 0.110*centers + 0.110*Local + 0.110*Community + 0.110*City + 0.110*councils + 0.110*Seattle + 0.010*piano + 0.010*play + 0.010*I', u'0.122*Seattle + 0.122*May + 0.122*Day + 0.122*riots + 0.122*political + 0.122*lot + 0.011*I + 0.011*piano + 0.011*play + 0.011*councils', u'0.033*Seattle + 0.033*I + 0.033*piano + 0.033*play + 0.033*May + 0.033*Day + 0.033*riots + 0.033*political + 0.033*lot + 0.033*councils']\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is the output of a batch LDA, which runs the entire corpus (in this case 5 very small txt files) through the model in one full pass, updates the model, then repeats this process. While online LDA is typically more computationally effecient, our corpus is so small that batch LDA works just as well. \n",
      "\n",
      "It may be immediately noticeable that the probability of all topics are small; in fact most topics have the same probability distribution of 0.033. In some instances topics are slightly more prominent, ranging from 0.061 to 0.122. The reason so many topics have such a small probability, as well as the same probability, can be attributed to the size and lack of variance in the data set. Should a larger and more diverse corpus had been used, topics might have more clearly emerged with fewer probabilities mirroring one another. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Calculating Jargon Distance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While LDA has successfully showed change in scientific fields over time, jargon distance peers deeper into the differences between scientific fields and examines \"cultural holes\" (http://www.knowledgelab.org/docs/finding-cultural-holes.pdf). Cultural holes widen as jargon increases in a particular group, essentially increasing the barrier to understanding for any outsiders. If jargon can be quantified, which the researchers who created the measurement formula believe it can, then scientists can work towards resolving unnecessary impediments in the language of their field. In summary, researchers who have studied the jargon distance of academic articles have concluded jargon isn't all that helpful or useful--and many students who are far from being experts in any field might be quick to agree!\n",
      "\n",
      "Jargon distance, and the replication of it in this particular example, is measuring the difference in probability distributions of a random variable (x). Given the data set, x could be any unigram found in a document within the corpus.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# JARGON DISTANCE\n",
      "# Look at the frequency of unigrams in a document\n",
      "# Calculate the probability of a unigram in the group of documents\n",
      "# Find probability of the unigram for the entire corpus\n",
      "# Calculate Shannon Entropy for a GROUP of documents\n",
      "# Then measure entropy across the 2 different groups, the \"cross_entropy\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_i = {'beautiful': 1, 'sound': 1, 'play': 1, 'sure': 1, 'like': 1, 'I': 1, 'computers': 1, 'problems': 1, 'Pianos': 2, 'solve': 1, 'piano': 1} #'City': 0, 'Seattle': 0, 'May': 0, 'familiar': 0, 'list': 0, 'Community': 0, 'centers': 0, 'riots': 0, 'councils': 0, 'officials': 0, 'candidates': 0, 'election': 0, 'lot': 0, 'political': 0, 'The': 0, '2016': 0, 'Local': 0, 'Day': 0, 'presidential': 0}\n",
      "\n",
      "data_j = {'City': 1, 'Seattle': 2, 'May': 1, 'familiar': 1, 'list': 1, 'Community': 1, 'centers': 1, 'riots': 1, 'councils': 1, 'officials': 1, 'candidates': 1, 'election': 1, 'lot': 1, 'political': 1, 'The': 1, '2016': 1, 'Local': 1, 'Day': 1, 'presidential': 1}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Frequency Distribution for Group i \n",
      "freqDist1 = FreqDist(data_i)\n",
      "print freqDist1.items()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('beautiful', 1), ('sound', 1), ('play', 1), ('sure', 1), ('like', 1), ('I', 1), ('computers', 1), ('problems', 1), ('Pianos', 2), ('solve', 1), ('piano', 1)]\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Probability Distribution for Group i\n",
      "probDist_i = []\n",
      "total_words = float(sum(freqDist1.values()))\n",
      "for key in freqDist1.keys():\n",
      "    freqDist1[key] = freqDist1[key]/total_words\n",
      "    probDist_i.append(freqDist1[key])\n",
      "print probDist_i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.08333333333333333, 0.08333333333333333, 0.08333333333333333, 0.08333333333333333, 0.08333333333333333, 0.08333333333333333, 0.08333333333333333, 0.08333333333333333, 0.16666666666666666, 0.08333333333333333, 0.08333333333333333]\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Frequency Distribution for Group j\n",
      "freqDist2 = FreqDist(data_j)\n",
      "print freqDist2.items()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('City', 1), ('councils', 1), ('familiar', 1), ('officials', 1), ('election', 1), ('political', 1), ('The', 1), ('Local', 1), ('centers', 1), ('presidential', 1), ('Seattle', 2), ('May', 1), ('list', 1), ('Community', 1), ('riots', 1), ('candidates', 1), ('lot', 1), ('2016', 1), ('Day', 1)]\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Probability Distribution for Group j\n",
      "probDist_j = []\n",
      "total_words_j = float(sum(freqDist2.values()))\n",
      "for key in freqDist2.keys():\n",
      "    freqDist2[key] = freqDist2[key]/total_words_j\n",
      "    probDist_j.append(freqDist2[key])\n",
      "print probDist_j"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus_data = {'beautiful': 1, 'sound': 1, 'play': 1, 'sure': 1, 'like': 1, 'I': 1, 'computers': 1, 'problems': 1, 'Pianos': 2, 'The': 1, 'solve': 1, 'piano': 1,'City': 1, 'councils': 1, 'familiar': 1, 'officials': 1, 'election': 1, 'political': 1, 'Local': 1, 'centers': 1, 'presidential': 1, 'Seattle': 2, 'May': 1, 'list': 1, 'Community': 1, 'riots': 1, 'candidates': 1, 'lot': 1, '2016': 1, 'Day': 1}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Frequency Distribution for Corpus\n",
      "freqDist3 = FreqDist(corpus_data)\n",
      "print freqDist3.items()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('beautiful', 1), ('City', 1), ('play', 1), ('sure', 1), ('councils', 1), ('familiar', 1), ('problems', 1), ('list', 1), ('officials', 1), ('election', 1), ('Pianos', 2), ('The', 1), ('Local', 1), ('centers', 1), ('presidential', 1), ('sound', 1), ('Seattle', 2), ('like', 1), ('May', 1), ('computers', 1), ('political', 1), ('Community', 1), ('riots', 1), ('I', 1), ('candidates', 1), ('lot', 1), ('solve', 1), ('piano', 1), ('2016', 1), ('Day', 1)]\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# Probability Distribution for Corpus\n",
      "probDist_corpus = []\n",
      "total_words_corp = float(sum(freqDist3.values()))\n",
      "for key in freqDist3.keys():\n",
      "    freqDist3[key] = freqDist3[key]/float(total_words_corp)\n",
      "    probDist_corpus.append(freqDist3[key])\n",
      "print probDist_corpus"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.0625, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.0625, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125]\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CODEBOOK PSI\n",
      "for key, value in freqDist3.items(): \n",
      "    if key not in freqDist1.keys(): \n",
      "        freqDist1[key] = value\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CODEBOOK PSJ\n",
      "for key, value in freqDist3.items(): \n",
      "    if key not in freqDist2.keys(): \n",
      "        freqDist2[key] = value"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# UPDATED PROBABILITIES \n",
      "print freqDist1.items()\n",
      "print\n",
      "print freqDist2.items()\n",
      "print\n",
      "print freqDist3.items()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('beautiful', 0.08333333333333333), ('City', 0.03125), ('councils', 0.03125), ('officials', 0.03125), ('election', 0.03125), ('Local', 0.03125), ('centers', 0.03125), ('May', 0.03125), ('political', 0.03125), ('riots', 0.03125), ('candidates', 0.03125), ('lot', 0.03125), ('piano', 0.08333333333333333), ('2016', 0.03125), ('Day', 0.03125), ('play', 0.08333333333333333), ('sure', 0.08333333333333333), ('I', 0.08333333333333333), ('familiar', 0.03125), ('problems', 0.08333333333333333), ('The', 0.03125), ('presidential', 0.03125), ('sound', 0.08333333333333333), ('like', 0.08333333333333333), ('Pianos', 0.16666666666666666), ('computers', 0.08333333333333333), ('list', 0.03125), ('Community', 0.03125), ('solve', 0.08333333333333333), ('Seattle', 0.0625)]\n",
        "\n",
        "[('beautiful', 0.03125), ('City', 0.05), ('play', 0.03125), ('sure', 0.03125), ('councils', 0.05), ('familiar', 0.05), ('problems', 0.03125), ('officials', 0.05), ('election', 0.05), ('political', 0.05), ('The', 0.05), ('Local', 0.05), ('centers', 0.05), ('presidential', 0.05), ('sound', 0.03125), ('Seattle', 0.1), ('I', 0.03125), ('like', 0.03125), ('May', 0.05), ('computers', 0.03125), ('list', 0.05), ('Community', 0.05), ('riots', 0.05), ('Pianos', 0.0625), ('candidates', 0.05), ('lot', 0.05), ('solve', 0.03125), ('piano', 0.03125), ('2016', 0.05), ('Day', 0.05)]\n",
        "\n",
        "[('beautiful', 0.03125), ('City', 0.03125), ('play', 0.03125), ('sure', 0.03125), ('councils', 0.03125), ('familiar', 0.03125), ('problems', 0.03125), ('list', 0.03125), ('officials', 0.03125), ('election', 0.03125), ('Pianos', 0.0625), ('The', 0.03125), ('Local', 0.03125), ('centers', 0.03125), ('presidential', 0.03125), ('sound', 0.03125), ('Seattle', 0.0625), ('like', 0.03125), ('May', 0.03125), ('computers', 0.03125), ('political', 0.03125), ('Community', 0.03125), ('riots', 0.03125), ('I', 0.03125), ('candidates', 0.03125), ('lot', 0.03125), ('solve', 0.03125), ('piano', 0.03125), ('2016', 0.03125), ('Day', 0.03125)]\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that the probability distribution of both document groups, or \"codebooks,\" have been calculated, we can begin to calculate entropy. Shannon entropy is the result of the expected message length per phrase (a unigram in this case) given the probability distribution of a random variable (x). First, we compute the optimum message length in the first group of documents, group i.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Shannon Entropy Code Citation: http://code.activestate.com/recipes/577476-shannon-entropy-calculation/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "shannon_entropy_i = -sum([x * np.log2(x) for x in freqDist1.itervalues()])\n",
      "print 'Shannon Entropy Group i'\n",
      "print shannon_entropy_i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Shannon Entropy Group i\n",
        "6.48079583405\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "shannon_entropy_groupj = -sum([x * np.log2(x) for x in freqDist2.itervalues()])\n",
      "print 'Shannon Entropy Group j'\n",
      "print shannon_entropy_groupj"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Shannon Entropy Group j\n",
        "6.03442809489\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cross_entropy(probDist1, probDist2):\n",
      "    x = []\n",
      "    for k, v in freqDist1.iteritems():\n",
      "        x.append(freqDist1[k] * np.log2(freqDist2[k]))\n",
      "    cross_entropy = -sum(x) \n",
      "    return cross_entropy\n",
      "XEntropy = cross_entropy(freqDist1.items(), freqDist2.items())\n",
      "print 'Cross Entropy from Group i to j'\n",
      "print XEntropy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cross Entropy from Group i to j\n",
        "7.47203839264\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can conclude a few things from the Shannon Entropy, the optimum message length of a random variable. First, the output of Shannon Entropy gives us the minimum number of bits, if rounded up, required to encode each symbol from group i, which is 7 bits for this example. (Note that entropy is rounded up since bits must be expressed in whole numbers only.) \n",
      "\n",
      "Cross Entropy, measuring messages between different groups discussing different topics results in a larger entropy or longer message length. This means a writer and reader from different fields will need to use longer messages (and more bits) to communicate and achieve a similar level of understanding. Intuitively, when people are from different groups and we see entropy  increase, this correlates to increasing jargon distance and more costly communication.\n",
      "\n",
      "It is interesting to note that evaluating cultural holes will vary by the direction of comparison. For example, comparing group i to group j will result in a different answer than comparing group j to group i. This can be interpreted as one group having less jargon, a smaller cultural hole, and easier to understand language for outsiders. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Calculating Efficency of Communication"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eff_comm = (shannon_entropy_i/XEntropy)\n",
      "print 'Efficiency Distance:', eff_comm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Efficiency Distance: 0.867339739641\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our data set is not ideal for this particular type of Jargon Distance measurement, and this is evident from near perfect communication efficiency--even across subject matter fields. Regardless, we can still see somewhat of a point demonstrated: jargon does exist and it impedes perfect communication. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Calculating a Cultural Hole"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hole = 1 - eff_comm\n",
      "print 'Cultural Hole:',hole"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cultural Hole: 0.132660260359\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The final step of this white paper process is to calculate the cultural hole between our 2 groups, which is equivalent to about 13%. If this result was true of a larger corpus, researchers may be able to conclude a great deal of effort had been put into the writing to eliminate jargon and invite outsiders into the discussion. \n",
      "\n",
      "Therein lies the major difference between computing LDA and the Cultural Hole of text-based data. LDA will cluster text into topics and produce the probability of a word's fitting into a particular topic, but measuring Cultural Holes looks at the desparity between documents. Cultural Holes and Jargon Distance care more about the collective whole--an entire document, a group of documents--and how these groups compare to one another. \n",
      "\n",
      "While LDA has been successful in measuring change of scientific fields, probably by measuring what topics and words are appearing most frequently over time, Jargon Distance measures how these words compare across fields. This becomes practical and purposeful when scientists, while aware they are progressing in the research and understanding of their own field, lose sight of how siloed and insular they are becoming to their larger scientific network or sphere of influence. Becoming aware of the use of jargon and trying to limit its appearance may stand to attract more people to a field that has previously been unapproachable or has grown increasingly complex over the years. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}