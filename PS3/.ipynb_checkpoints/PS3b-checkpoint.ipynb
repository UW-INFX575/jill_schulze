{
 "metadata": {
  "name": "",
  "signature": "sha256:d3b677c6218b6812afe3bd97e6aa30f5103f69a656756a2f492e21bee076d08b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "PS3b: Scaling Jargon Distance for Large Corpus"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Jill Schulze - May 21, 2015"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import libraries\n",
      "import csv\n",
      "import nltk\n",
      "import time\n",
      "import math\n",
      "import numpy as np\n",
      "# from nltk.probability import *\n",
      "import matplotlib.pylab as plt\n",
      "from matplotlib.pyplot import show\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.probability import ConditionalFreqDist\n",
      "from scipy.cluster.hierarchy import dendrogram \n",
      "from scipy.cluster.hierarchy import linkage"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# \"Word fluff\" stopwords to be removed\n",
      "stopwords = [\"all\",\"just\",\"being\",\"over\",\"both\",\"through\",\"yourselves\",\"its\",\"before\",\"herself\",\n",
      "            \"had\",\"should\",\"to\",\"only\",\"under\",\"ours\",\"has\",\"do\",\"them\",\"his\",\"very\",\"they\",\"not\",\n",
      "            \"during\",\"now\",\"him\",\"nor\",\"did\",\"this\",\"she\",\"each\",\"further\",\"where\",\"few\",\"because\",\n",
      "            \"doing\",\"some\",\"are\",\"our\",\"ourselves\",\"out\",\"what\",\"for\",\"while\",\"does\",\"above\",\"between\",\n",
      "            \"t\",\"be\",\"we\",\"who\",\"were\",\"here\",\"hers\",\"by\",\"on\",\"about\",\"of\",\"against\",\"s\",\"or\",\"own\",\n",
      "            \"into\",\"yourself\",\"down\",\"your\",\"from\",\"her\",\"their\",\"there\",\"been\",\"whom\",\"too\",\"themselves\",\n",
      "            \"was\",\"until\",\"more\",\"himself\",\"that\",\"but\",\"don\",\"with\",\"than\",\"those\",\"he\",\"me\",\"myself\",\n",
      "            \"these\",\"up\",\"will\",\"below\",\"can\",\"theirs\",\"my\",\"and\",\"then\",\"is\",\"am\",\"it\",\"an\",\"as\",\"itself\",\n",
      "            \"at\",\"have\",\"in\",\"any\",\"if\",\"again\",\"no\",\"when\",\"same\",\"how\",\"other\",\"which\",\"you\",\"after\",\n",
      "            \"most\",\"such\",\"why\",\"a\",\"off\",\"i\",\"yours\",\"so\",\"the\",\"having\",\"once\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Begin timer\n",
      "timer = time.time() "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load file with text documents \n",
      "with open(\"abstracts2.txt\") as f:\n",
      "    reader = csv.reader(f, delimiter=\"\\t\")\n",
      "    abstracts = list(reader)\n",
      "\n",
      " \n",
      "# Make dictionary of doc id and abstract text \n",
      "abstracts_dict = {doc[0]:doc[1] for doc in abstracts}   \n",
      "\n",
      "# Check it\n",
      "#print abstracts_dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Load groups file\n",
      "with open(\"groups2.txt\") as f:\n",
      "    reader = csv.reader(f, delimiter=\"\\t\")\n",
      "    fields = list(reader)\n",
      "    \n",
      "#Make dictionary between doc and group\n",
      "fields_dict = {field[0]:field[1] for field in fields[1:]}\n",
      "\n",
      "# Check it\n",
      "#print fields_dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create a bucket for the 10 \"fields\" of subject matter\n",
      "fields = [] \n",
      "for item in range(0,10): \n",
      "    fields.append(list(key for key, value in fields_dict.iteritems() if int(value) == item + 1)) \n",
      "    \n",
      "    #Replace doc id with document using list comprehension\n",
      "    fields[item] = [abstracts_dict[random_x] for random_x in fields[item]] \n",
      "    fields[item] = [doc for doc in fields[item] if doc != \"null\"]\n",
      "\n",
      "# Check it\n",
      "#print fields\n",
      "#print len(fields) # There are 10 fields, which is correct"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Construct corpus of all docs\n",
      "corpus = list(doc for field in fields for doc in field)\n",
      "\n",
      "# Check it\n",
      "#print corpus"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Break field subject matter strings into substrings or \"bags of words,\" remove stopwords, & lowercase\n",
      "tokenizer = RegexpTokenizer(r'\\w+')\n",
      "\n",
      "tokens = [] \n",
      "for item in range(0,10): \n",
      "    tokens.append(tokenizer.tokenize(' '.join(fields[item])))\n",
      "    #print tokens\n",
      "    tokens[item] = [word.lower() for word in tokens[item] if word not in stopwords]  \n",
      "#print tokens\n",
      "\n",
      "# Chunk all the words or \"tokens\" together \n",
      "corpus_tokens = list(token for tokenset in tokens for token in tokenset)\n",
      "\n",
      "# Check it\n",
      "#corpus_tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Using NLTK.Probability, calculate the frequency distributions for each field of subject matter\n",
      "field_freqs_dict = [] \n",
      "for item in range(0,10): \n",
      "    field_freqs_dict.append(FreqDist(tokens[item])) \n",
      "#print field_freqs_dict\n",
      "\n",
      "# Now do the same for the entire corpus    \n",
      "corpus_freqs_dict = FreqDist(corpus_tokens) \n",
      "#corpus_freqs_dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Using the frequency distribution, calculate probability distributions for each field\n",
      "# Also known as \"codebooks\"\n",
      "# psi(x) = (1 \u2212 \u03b1)pi(x) + \u03b1s(x)\n",
      "\n",
      "# Corpus codebook - divide the frequency of each word by the word count of the corpus\n",
      "corpus_total = float(sum(corpus_freqs_dict.values())) \n",
      "for word in corpus_freqs_dict.keys(): \n",
      "    corpus_freqs_dict[word] = corpus_freqs_dict[word]/corpus_total"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Field codebooks - probability distribution for a word in the subject matter    \n",
      "# Tried writing as a for loop w/else statement, and values were skewed. TRY/EXCEPT works.\n",
      "# Try/except loop adapted from Anu Garimella's code!\n",
      "# psi(x) = (1 \u2212 \u03b1)pi(x) + \u03b1s(x)\n",
      "\n",
      "alpha = 0.01\n",
      "\n",
      "#field_codebooks = []    \n",
      "for field_dict in field_freqs_dict: \n",
      "    field_total = float(sum(field_dict.values())) \n",
      "    for word in corpus_freqs_dict: \n",
      "        try: \n",
      "            field_dict[word] = (1.0-alpha)*(field_dict[word]/field_total)+(alpha*corpus_freqs_dict[word])\n",
      "            #field_codebooks.append(field[word])\n",
      "        except KeyError, e:\n",
      "            field_dict[word] = alpha*corpus_freqs_dict[word]\n",
      "            #field_codebooks.append(field[word])\n",
      "#print field_codebooks            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calculate Shannon entropy for each field\n",
      "shannon_entropies = [] \n",
      "for item in range(0,10):\n",
      "    shannon_entropies.append(-1.0*sum(probDist*math.log(probDist,2) for probDist in field_freqs_dict[item].values()))\n",
      "\n",
      "# Check entropy values are different and not the same!    \n",
      "print shannon_entropies"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[12.440507132510934, 12.048236060174837, 11.820299069889847, 11.696919515312457, 11.251154161216615, 11.415737869871139, 11.334956397204461, 11.94775629149263, 11.166831872475095, 11.437396965392624]\n"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calculate X entropy and store each field in matrix\n",
      "# Matrix idea adapted from Anu Garimella's code!\n",
      "x_entropies = np.zeros((10,10))\n",
      "for i in range(0,10):\n",
      "    for j in range(0,10):\n",
      "        x_entropies[i][j] = -1.0*sum(field_freqs_dict[i][word]*math.log(field_freqs_dict[j][word],2)for word in corpus_freqs_dict.keys())  \n",
      "\n",
      "print x_entropies"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 12.44050713  14.91932195  15.3998222   15.2857777   15.39459973\n",
        "   15.87073289  16.53497814  15.80378541  16.0774831   15.72984423]\n",
        " [ 14.51638118  12.04823606  16.19877908  16.13598941  15.62763684\n",
        "   16.49023716  17.03648797  16.65510829  16.84069782  16.64158555]\n",
        " [ 14.47978774  15.66765493  11.82029907  14.03733169  14.37702708\n",
        "   14.02943638  14.96461063  14.42606024  14.63616776  14.64359527]\n",
        " [ 14.27755635  15.35152581  13.93098185  11.69691952  14.66422525\n",
        "   14.01905655  14.80439668  14.24634533  14.2285234   13.92443493]\n",
        " [ 13.54053449  14.25314711  13.298452    14.05161129  11.25115416\n",
        "   14.253691    15.17891449  14.77276972  14.66547501  14.32643608]\n",
        " [ 14.42871752  15.31962764  13.27535709  13.39596609  14.39693416\n",
        "   11.41573787  14.42685652  14.19293473  14.14485709  13.9635958 ]\n",
        " [ 15.43377425  16.39650583  14.22475704  14.26747189  15.46089283\n",
        "   14.70521906  11.3349564   14.62549526  14.80979907  15.16038724]\n",
        " [ 15.62273091  17.10348878  15.03896229  15.07552456  16.03624009\n",
        "   15.65936252  15.98512549  11.94775629  15.96552794  15.50477861]\n",
        " [ 14.58011258  15.64620017  13.534324    13.22210045  14.69197984\n",
        "   13.85348022  14.3153493   14.08004829  11.16683187  14.18374617]\n",
        " [ 14.72836214  16.11378327  14.32810757  13.80749465  15.0588862\n",
        "   14.50453198  15.52466188  14.63947996  14.99976805  11.43739697]]\n"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Build jargon distance matrix -- will be used later for dendrogram\n",
      "# Communication Efficiency & Jargon Distance matrix adapted from Anu Garimella's code!\n",
      "comm_efficiency = np.zeros((10,10))\n",
      "jargon_distance = np.zeros((10,10))\n",
      "\n",
      "# Store results in matrix\n",
      "for i in range(0,10):\n",
      "    for j in range(0,10):\n",
      "        comm_efficiency[i][j] = shannon_entropies[i]/x_entropies[i][j]\n",
      "        jargon_distance[i][j] = 1 - comm_efficiency[i][j]\n",
      "        if jargon_distance[i][j] < alpha:\n",
      "            jargon_distance[i][j] = 0\n",
      "\n",
      "# Review matrix            \n",
      "print jargon_distance"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.          0.16614795  0.19216553  0.18613842  0.19189148  0.21613531\n",
        "   0.24762482  0.21281473  0.22621551  0.20911441]\n",
        " [ 0.17002482  0.          0.25622567  0.25333144  0.229043    0.26937157\n",
        "   0.29279814  0.27660416  0.2845762   0.27601634]\n",
        " [ 0.18366904  0.24556042  0.          0.15793832  0.17783426  0.15746444\n",
        "   0.2101165   0.18062875  0.19239112  0.19280075]\n",
        " [ 0.1807478   0.23806144  0.16036647  0.          0.20234998  0.16564146\n",
        "   0.20990232  0.17895297  0.17792457  0.15997169]\n",
        " [ 0.16907607  0.21061966  0.15395009  0.19929794  0.          0.21064978\n",
        "   0.25876424  0.2383856   0.23281352  0.21465785]\n",
        " [ 0.20881826  0.25482929  0.14008054  0.14782273  0.20707161  0.\n",
        "   0.2087162   0.1956746   0.19294074  0.18246431]\n",
        " [ 0.26557456  0.30869683  0.2031529   0.20553855  0.26686275  0.2291882\n",
        "   0.          0.22498649  0.23463132  0.25233068]\n",
        " [ 0.23523254  0.30144332  0.2055465   0.20747326  0.25495277  0.23702154\n",
        "   0.25257038  0.          0.25165292  0.22941458]\n",
        " [ 0.23410524  0.28629113  0.17492504  0.15544191  0.23993689  0.1939331\n",
        "   0.21993997  0.20690387  0.          0.21270222]\n",
        " [ 0.22344407  0.29021033  0.20175104  0.17165299  0.24048852  0.21146046\n",
        "   0.26327562  0.21872929  0.23749508  0.        ]]\n"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# End timer\n",
      "print \"Done-zo! \" + str(time.time() - timer) + \" seconds\" "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done-zo! 26.2181839943 seconds\n"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Graph a dendrogram to show similarity and difference of fields\n",
      "linkage = linkage(jargon_distance, method = 'average')\n",
      "dendrogram(linkage)\n",
      "\n",
      "# Note that dendrogram will populate in separate window outsite of iPython notebook\n",
      "# You may want to minimize this notebook to locate the dendrogram\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'jargon_distance' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-9-7ea2fbafb9bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Graph a dendrogram to show similarity and difference of fields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlinkage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinkage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjargon_distance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'average'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdendrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinkage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Note that dendrogram will populate in separate window outsite of iPython notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'jargon_distance' is not defined"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Minimize iPython notebook to see dendrogram.\" "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}